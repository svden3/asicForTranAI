% Key LLM papers
@article{achiam2023gpt4,
  title={GPT-4 Technical Report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Gemini Team and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{meta2024llama3,
  title={The Llama 3 Herd of Models},
  author={Meta AI},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% Quantization methods
@article{frantar2023gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2023}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and others},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}

@article{dettmers2022llm8bit,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{xiao2023smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and others},
  journal={arXiv preprint arXiv:2211.10438},
  year={2023}
}

@article{jacob2018quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and others},
  journal={CVPR},
  year={2018}
}

@article{zhang2020ternary,
  title={Ternary Weight Networks},
  author={Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
  journal={CVPR},
  year={2020}
}

% ASIC hardware
@article{groq2024lpu,
  title={Groq LPU: A High-Performance Language Processing Unit},
  author={Groq Inc.},
  journal={White Paper},
  year={2024},
  url={https://groq.com}
}

@article{cerebras2024wse4,
  title={Cerebras Wafer-Scale Engine 4: The Largest Chip Ever Built},
  author={Cerebras Systems},
  journal={White Paper},
  year={2024},
  url={https://cerebras.net}
}

@article{jouppi2017tpu,
  title={In-Datacenter Performance Analysis of a Tensor Processing Unit},
  author={Jouppi, Norman P. and Young, Cliff and Patil, Nishant and others},
  journal={ISCA},
  year={2017}
}

% Formal verification
@article{katz2017reluplex,
  title={Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks},
  author={Katz, Guy and Barrett, Clark and Dill, David L. and others},
  journal={CAV},
  year={2017}
}

@article{wang2018formal,
  title={Formal Security Analysis of Neural Networks using Symbolic Intervals},
  author={Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and others},
  journal={USENIX Security},
  year={2018}
}

@article{leroy2009compcert,
  title={Formal Verification of a Realistic Compiler},
  author={Leroy, Xavier},
  journal={Communications of the ACM},
  volume={52},
  number={7},
  pages={107--115},
  year={2009}
}

@article{kumar2014cakeml,
  title={CakeML: A Verified Implementation of ML},
  author={Kumar, Ramana and Myreen, Magnus O. and Norrish, Michael and Owens, Scott},
  journal={POPL},
  year={2014}
}

% Benchmarks
@article{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and others},
  journal={ICLR},
  year={2021}
}

@article{chen2021humaneval,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{lin2022truthfulqa,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={ACL},
  year={2022}
}
