\begin{table}[t]
\centering
\caption{Memory footprint comparison across quantization schemes for LLaMA models.
Our 3.5-bit scheme achieves 46\% reduction vs INT4 while maintaining accuracy.}
\label{tab:memory}
\begin{tabular}{lrrrr}
\toprule
Model & FP16 (GB) & INT8 (GB) & INT4 (GB) & \textbf{3.5-bit (GB)} \\
\midrule
LLaMA-7B & 14.0 & 7.0 & 3.5 & \textbf{3.1} \\
LLaMA-13B & 26.0 & 13.0 & 6.5 & \textbf{5.7} \\
LLaMA-70B & 140.0 & 70.0 & 35.0 & \textbf{30.6} \\
LLaMA-405B & 810.0 & 405.0 & 202.5 & \textbf{177.2} \\
\midrule
Reduction vs FP16 & -- & 50\% & 75\% & \textbf{78.1\%} \\
Reduction vs INT4 & -- & -- & -- & \textbf{46.4\%} \\
\bottomrule
\end{tabular}
\end{table}
