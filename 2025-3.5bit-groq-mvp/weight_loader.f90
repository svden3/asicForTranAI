! Weight Loader for LLaMA 70B Model
! Reads binary weight files generated by Python scripts or test generator
! Pure Fortran 2023

module weight_loader
    use iso_fortran_env, only: int8, int32, real32
    use transformer_layer
    implicit none

    private
    public :: load_layer_weights

contains

    !===========================================================================
    ! Load weights for a single transformer layer from binary file
    !===========================================================================
    subroutine load_layer_weights(layer, filename, layer_idx)
        type(TransformerLayer), intent(inout) :: layer
        character(len=*), intent(in) :: filename
        integer(int32), intent(in) :: layer_idx

        integer :: unit, ios
        character(len=256) :: error_msg

        print '(A,I0,A)', "Loading layer ", layer_idx, " weights..."

        ! Open binary file
        open(newunit=unit, file=trim(filename), form='unformatted', &
             access='stream', status='old', action='read', iostat=ios)

        if (ios /= 0) then
            write(error_msg, '(A,A)') "Error opening file: ", trim(filename)
            print *, trim(error_msg)
            return
        end if

        ! Allocate weight arrays if not already allocated
        if (.not. allocated(layer%wq)) then
            allocate(layer%wq(HIDDEN_DIM/8, NUM_HEADS * HEAD_DIM))
            allocate(layer%wq_scales(NUM_HEADS * HEAD_DIM))
        end if

        if (.not. allocated(layer%wk)) then
            allocate(layer%wk(HIDDEN_DIM/8, NUM_KV_HEADS * HEAD_DIM))
            allocate(layer%wk_scales(NUM_KV_HEADS * HEAD_DIM))
        end if

        if (.not. allocated(layer%wv)) then
            allocate(layer%wv(HIDDEN_DIM/8, NUM_KV_HEADS * HEAD_DIM))
            allocate(layer%wv_scales(NUM_KV_HEADS * HEAD_DIM))
        end if

        if (.not. allocated(layer%wo)) then
            allocate(layer%wo(HIDDEN_DIM/8, HIDDEN_DIM))
            allocate(layer%wo_scales(HIDDEN_DIM))
        end if

        if (.not. allocated(layer%w_gate)) then
            allocate(layer%w_gate(HIDDEN_DIM/8, INTERMEDIATE_DIM))
            allocate(layer%w_gate_scales(INTERMEDIATE_DIM))
        end if

        if (.not. allocated(layer%w_up)) then
            allocate(layer%w_up(HIDDEN_DIM/8, INTERMEDIATE_DIM))
            allocate(layer%w_up_scales(INTERMEDIATE_DIM))
        end if

        if (.not. allocated(layer%w_down)) then
            allocate(layer%w_down(INTERMEDIATE_DIM/8, HIDDEN_DIM))
            allocate(layer%w_down_scales(HIDDEN_DIM))
        end if

        ! Read weights in the same order they were written
        ! (Must match generate_test_weights.f90 and convert_weights_to_fortran.py)

        ! 1. Q projection
        read(unit, iostat=ios) layer%wq
        if (ios /= 0) then
            print *, "  ✗ Error reading wq"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%wq_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading wq_scales"
            close(unit)
            return
        end if

        ! 2. K projection
        read(unit, iostat=ios) layer%wk
        if (ios /= 0) then
            print *, "  ✗ Error reading wk"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%wk_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading wk_scales"
            close(unit)
            return
        end if

        ! 3. V projection
        read(unit, iostat=ios) layer%wv
        if (ios /= 0) then
            print *, "  ✗ Error reading wv"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%wv_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading wv_scales"
            close(unit)
            return
        end if

        ! 4. O projection
        read(unit, iostat=ios) layer%wo
        if (ios /= 0) then
            print *, "  ✗ Error reading wo"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%wo_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading wo_scales"
            close(unit)
            return
        end if

        ! 5. Gate projection
        read(unit, iostat=ios) layer%w_gate
        if (ios /= 0) then
            print *, "  ✗ Error reading w_gate"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%w_gate_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading w_gate_scales"
            close(unit)
            return
        end if

        ! 6. Up projection
        read(unit, iostat=ios) layer%w_up
        if (ios /= 0) then
            print *, "  ✗ Error reading w_up"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%w_up_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading w_up_scales"
            close(unit)
            return
        end if

        ! 7. Down projection
        read(unit, iostat=ios) layer%w_down
        if (ios /= 0) then
            print *, "  ✗ Error reading w_down"
            close(unit)
            return
        end if
        read(unit, iostat=ios) layer%w_down_scales
        if (ios /= 0) then
            print *, "  ✗ Error reading w_down_scales"
            close(unit)
            return
        end if

        close(unit)

        print '(A,I0,A)', "  ✓ Layer ", layer_idx, " weights loaded successfully"
        print '(A,I0,A)', "    Total weights: ", &
            size(layer%wq) + size(layer%wk) + size(layer%wv) + size(layer%wo) + &
            size(layer%w_gate) + size(layer%w_up) + size(layer%w_down), " values"

    end subroutine load_layer_weights

    ! Note: load_all_model_weights() function for loading all 80 layers
    ! will be added to llama_model.f90 to avoid circular dependencies

end module weight_loader
